model:
  vocab_size: 50257
  context_length: 512
  emb_dim: 256
  n_heads: 4
  n_layers: 12
  drop_rate: 0.05  # Reduced dropout for fine-tuning
  qkv_bias: false

train:
  batch_size: 4                    
  lr: 0.0002                       
  weight_decay: 0.001              
  num_epochs: 3
  eval_freq: 1000
  eval_iter: 50
  grad_accum_steps: 4              # Effective batch size = 16
  amp: true
  device: cuda
  seed: 123
  num_workers: 2
  warmup_steps: 200                # Much shorter warmup for fine-tuning
  min_lr: 1.0e-6
  betas: [0.9, 0.999]
  eps: 1.0e-8
  fused: true
  grad_clip_norm: 1.0