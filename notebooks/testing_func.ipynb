{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a6fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0adf2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_repetition_penalty(\n",
    "    logits: torch.Tensor, generated: torch.Tensor, penalty: float\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    if penalty <= 1.0 or generated.numel() == 0:\n",
    "        return logits\n",
    "    \n",
    "    unique_tokens = torch.unique(generated)\n",
    "    print(f\"unique_tokens: {unique_tokens}\")\n",
    "    token_indices = unique_tokens.long()\n",
    "    print(f\"token_indices: {token_indices}\")\n",
    "    \n",
    "    # Get the logits for the repeated tokens\n",
    "    vals = logits[0, token_indices]\n",
    "    print(f\"vals: {vals}\")\n",
    "    \n",
    "    pos_mask = vals > 0\n",
    "    print(f\"pos_mask: {pos_mask}\")\n",
    "    \n",
    "    vals = torch.where(pos_mask, vals / penalty, vals * penalty)\n",
    "    print(f\"vals: {vals}\")\n",
    "\n",
    "    logits[0, token_indices] = vals\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788fe340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0000,  3.0000,  3.0000,  0.5000, -1.0000,  2.5000,  1.0000,  0.0000,\n",
      "         -0.5000,  1.8000]])\n",
      "tensor([[1, 3, 2, 1, 8]])\n",
      "unique_tokens: tensor([1, 2, 3, 8])\n",
      "token_indices: tensor([1, 2, 3, 8])\n",
      "vals: tensor([ 3.0000,  3.0000,  0.5000, -0.5000])\n",
      "pos_mask: tensor([ True,  True,  True, False])\n",
      "vals: tensor([ 2.0000,  2.0000,  0.3333, -0.7500])\n",
      "tensor([[ 2.0000,  2.0000,  2.0000,  0.3333, -1.0000,  2.5000,  1.0000,  0.0000,\n",
      "         -0.7500,  1.8000]])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "penalty = 1.5\n",
    "\n",
    "original_logits = torch.tensor([[2.0, 3.0, 3.0, 0.5, -1.0, 2.5, 1.0, 0.0, -0.5, 1.8]])\n",
    "print(original_logits)\n",
    "\n",
    "generated_tokens = torch.tensor([[1, 3, 2, 1, 8]])\n",
    "print(generated_tokens)\n",
    "\n",
    "penalized_logits = _apply_repetition_penalty(original_logits, generated_tokens, penalty)\n",
    "print(penalized_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9589d4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: [1, 2, 3, 1, 2, 4]\n",
      "prefix: (1,), nxt: 2\n",
      "prefix: (2,), nxt: 3\n",
      "prefix: (3,), nxt: 1\n",
      "prefix: (1,), nxt: 2\n",
      "prefix: (2,), nxt: 4\n",
      "final prefix map: {(1,): [2, 2], (2,): [3, 4], (3,): [1]}\n",
      "Current prefix (last 1 tokens): (4,)\n",
      "Banned tokens for current prefix: []\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple, List\n",
    "\n",
    "def _enforce_no_repeat_ngram(\n",
    "    logits: torch.Tensor, generated: torch.Tensor, no_repeat_ngram_size: int\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    if no_repeat_ngram_size <= 0:\n",
    "        return logits\n",
    "\n",
    "    seq = generated.squeeze(0).tolist()\n",
    "    print(f\"seq: {seq}\")\n",
    "    \n",
    "    if len(seq) < no_repeat_ngram_size - 1:\n",
    "        return logits\n",
    "\n",
    "    prefix_len = no_repeat_ngram_size - 1\n",
    "    \n",
    "    # Build map: prefix -> next_token set\n",
    "    next_for_prefix: Dict[Tuple[int, ...], List[int]] = {}\n",
    "    \n",
    "    for i in range(len(seq) - no_repeat_ngram_size + 1):\n",
    "        prefix = tuple(seq[i : i + prefix_len])\n",
    "        nxt = seq[i + prefix_len]\n",
    "        print(f\"prefix: {prefix}, nxt: {nxt}\")\n",
    "        next_for_prefix.setdefault(prefix, []).append(nxt)\n",
    "\n",
    "    print(f\"final prefix map: {next_for_prefix}\")\n",
    "    \n",
    "    cur_prefix = tuple(seq[-prefix_len:])\n",
    "    print(f\"Current prefix (last {prefix_len} tokens): {cur_prefix}\")\n",
    "    \n",
    "    banned = next_for_prefix.get(cur_prefix, [])\n",
    "    print(f\"Banned tokens for current prefix: {banned}\")\n",
    "    if banned:\n",
    "        print(f\"Blocking tokens: {banned}\")\n",
    "        banned_idx = torch.tensor(\n",
    "            banned, device=logits.device, dtype=torch.long\n",
    "        ).unsqueeze(0)\n",
    "        print(f\"Banned indices tensor: {banned_idx}\")\n",
    "        \n",
    "        \n",
    "        # Show logits before modification\n",
    "        print(f\"Logits before blocking: {logits}\")\n",
    "        logits = logits.scatter(dim=-1, index=banned_idx, value=float(\"-inf\"))\n",
    "        print(f\"Logits after blocking: {logits}\")\n",
    "        \n",
    "    return logits\n",
    "\n",
    "vocab_size = 10\n",
    "generated = torch.tensor([[1, 2, 3, 1, 2, 4]])\n",
    "logits = torch.randn(1, vocab_size)\n",
    "\n",
    "result = _enforce_no_repeat_ngram(logits, generated, 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
