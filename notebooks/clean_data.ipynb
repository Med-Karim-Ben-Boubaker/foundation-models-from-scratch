{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350b4798",
   "metadata": {},
   "source": [
    "### BabyLM data preprocessing for GPT-2 pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4ee72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents: 6\n",
      "document length (chars): 8411630\n",
      "document length (chars): 13910986\n",
      "document length (chars): 4883879\n",
      "document length (chars): 719322\n",
      "document length (chars): 10806305\n",
      "document length (chars): 15482927\n",
      "Memory usage: 118.77 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "training_data_folder_path = \"../data/train_10M\"\n",
    "\n",
    "output_file = \"../data/training_data_cleaned.txt\"\n",
    "\n",
    "file_paths = glob.glob(os.path.join(training_data_folder_path, \"*\"))\n",
    "contents: list[str] = []\n",
    "for file_path in file_paths:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            contents.append(f.read())\n",
    "            \n",
    "print(f\"number of documents: {len(contents)}\")\n",
    "\n",
    "size = 0\n",
    "for doc in contents:\n",
    "    print(f\"document length (chars): {len(doc)}\")\n",
    "    size += sys.getsizeof(doc)\n",
    "    \n",
    "print(f\"Memory usage: {size / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddc86d",
   "metadata": {},
   "source": [
    "#### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea70e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning document 1/6...\n",
      "  Pattern '^= = = .* = = =$': removed 12284 matches\n",
      "  Pattern '^-\\s*': removed 26 matches\n",
      "  Pattern '\\[.*?\\]': removed 179 matches\n",
      "  Pattern '^.*\\s{4,}.*$': removed 945 matches\n",
      "  Short lines: removed 26421 lines shorter than 10 characters\n",
      "Document 1 cleaned. Original: 8411630 chars, Cleaned: 7582309 chars\n",
      "Cleaning document 2/6...\n",
      "  Pattern '^= = = .* = = =$': removed 50 matches\n",
      "  Pattern '^-\\s*': removed 102 matches\n",
      "  Pattern '^\\*[^*]+\\*$': removed 55 matches\n",
      "  Pattern '\\[.*?\\]': removed 2203 matches\n",
      "  Pattern '^.*\\s{4,}.*$': removed 1233 matches\n",
      "  Short lines: removed 2638 lines shorter than 10 characters\n",
      "Document 2 cleaned. Original: 13910986 chars, Cleaned: 13697692 chars\n",
      "Cleaning document 3/6...\n",
      "  Pattern '^.*\\s{4,}.*$': removed 268 matches\n",
      "  Short lines: removed 22086 lines shorter than 10 characters\n",
      "Document 3 cleaned. Original: 4883879 chars, Cleaned: 4725138 chars\n",
      "Cleaning document 4/6...\n",
      "  Pattern '^[A-Z]:\\s*': removed 18000 matches\n",
      "  Short lines: removed 6172 lines shorter than 10 characters\n",
      "Document 4 cleaned. Original: 719322 chars, Cleaned: 623897 chars\n",
      "Cleaning document 5/6...\n",
      "  Pattern '^-\\s*': removed 41927 matches\n",
      "  Pattern '^[A-Z]:\\s*': removed 10 matches\n",
      "  Pattern '^\\*[^*]+\\*$': removed 122 matches\n",
      "  Pattern '\\[.*?\\]': removed 2821 matches\n",
      "  Pattern '^.*\\s{4,}.*$': removed 138 matches\n",
      "  Short lines: removed 59440 lines shorter than 10 characters\n",
      "Document 5 cleaned. Original: 10806305 chars, Cleaned: 9979306 chars\n",
      "Cleaning document 6/6...\n",
      "  Pattern '^= = = .* = = =$': removed 960 matches\n",
      "  Pattern '^\\*\\w+:\\s*': removed 537662 matches\n",
      "  Pattern '\\[.*?\\]': removed 69214 matches\n",
      "  Pattern '^.*\\s{4,}.*$': removed 1812 matches\n",
      "  Short lines: removed 223954 lines shorter than 10 characters\n",
      "Document 6 cleaned. Original: 15482927 chars, Cleaned: 9283034 chars\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Updated patterns including removal of lines with excessive spacing\n",
    "patterns = [\n",
    "    r'^= = = .* = = =$',        # Remove lines like \"= = = Corcelles-près-Payerne = = =\"\n",
    "    r'^-\\s*',                   # Remove \"- \" from the beginning of lines\n",
    "    r'^[A-Z]:\\s*',             # Remove speaker labels like \"A:\", \"B:\" (with space or tab)\n",
    "    r'^\\*[^*]+\\*$',            # Remove chapter headers like \"*CHAPTER XI*\"\n",
    "    r'^\\*\\w+:\\s*',             # Remove speaker labels like \"*CHI:\" (with space or tab)\n",
    "    r'\\[.*?\\]',                # Remove any content between square brackets [anything]\n",
    "    r'^.*\\s{4,}.*$',           # Remove lines with 4 or more consecutive spaces\n",
    "]\n",
    "\n",
    "# Function to remove short lines (less than 10 characters of actual content)\n",
    "def remove_short_lines(text: str, min_length: int = 10) -> str:\n",
    "    \"\"\"Remove lines that have less than min_length characters of actual content.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Strip whitespace and check actual content length\n",
    "        content = line.strip()\n",
    "        if len(content) >= min_length:\n",
    "            filtered_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "# Apply cleaning to each document and store the results\n",
    "cleaned_contents = []\n",
    "for i, doc in enumerate(contents):\n",
    "    print(f\"Cleaning document {i+1}/{len(contents)}...\")\n",
    "    cleaned_doc = doc\n",
    "    \n",
    "    # Apply regex patterns\n",
    "    for pattern in patterns:\n",
    "        # Count matches before cleaning\n",
    "        matches_before = len(re.findall(pattern, cleaned_doc, flags=re.MULTILINE))\n",
    "        cleaned_doc = re.sub(pattern, '', cleaned_doc, flags=re.MULTILINE)\n",
    "        matches_after = len(re.findall(pattern, cleaned_doc, flags=re.MULTILINE))\n",
    "        \n",
    "        if matches_before > 0:\n",
    "            print(f\"  Pattern '{pattern}': removed {matches_before} matches\")\n",
    "    \n",
    "    # Remove short lines\n",
    "    lines_before = len(cleaned_doc.split('\\n'))\n",
    "    cleaned_doc = remove_short_lines(cleaned_doc, min_length=10)\n",
    "    lines_after = len(cleaned_doc.split('\\n'))\n",
    "    short_lines_removed = lines_before - lines_after\n",
    "    \n",
    "    if short_lines_removed > 0:\n",
    "        print(f\"  Short lines: removed {short_lines_removed} lines shorter than 10 characters\")\n",
    "    \n",
    "    cleaned_contents.append(cleaned_doc)\n",
    "    print(f\"Document {i+1} cleaned. Original: {len(doc)} chars, Cleaned: {len(cleaned_doc)} chars\")\n",
    "\n",
    "# Update the contents list with cleaned data\n",
    "contents = cleaned_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd97553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cleaned text to a new file\n",
    "final_text = \"\"\n",
    "for doc in contents:\n",
    "    final_text += doc\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a3bed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding <|endoftext|> tokens to final_text (treating each line as a sentence)...\n",
      "Original text length: 45891376 characters\n",
      "Text with special tokens length: 57133118 characters\n",
      "Number of sentences (lines): 815911\n",
      "Number of tokens with special tokens: 13784480\n",
      "\n",
      "Sample of first 5 sentences with <|endoftext|> tokens:\n",
      "1. The usage of .gov as a gTLD controlled only by the U.S. is controversial, as some people believe this to be an example of arrogance by the U.S. – such views hold that usage of .fed.us or a new second-level domain of .gov.us would be more suitable. Others believe that U.S. control of .gov is a natural result of the fact that the U.S. federal government was responsible for the initial creation of the Internet and its first user. <|endoftext|>\n",
      "2. Corcelles-près-Payerne is a municipality in the Broye-Vully district in the canton of Vaud in Switzerland. <|endoftext|>\n",
      "3. Cremin was a municipality of the district of Broye-Vully in the canton of Vaud in Switzerland. On 1 January 2017, the former municipalities of Brenles, Chesalles-sur-Moudon, Cremin, Forel-sur-Lucens and Sarzens joined together to become the municipality of Lucens. <|endoftext|>\n",
      "4. Lucens is a municipality in Broye-Vully in the canton of Vaud in Switzerland. <|endoftext|>\n",
      "5. Lucens was first mentioned in 964 as \"in villa Losingus\". It was formerly known by the German name \"Losingen\". <|endoftext|>\n",
      "\n",
      "Special token sample: 'Hello world. <|endoftext|>'\n",
      "Tokenized as: [15496, 995, 13, 220, 50256]\n",
      "Decoded back: 'Hello world. <|endoftext|>'\n"
     ]
    }
   ],
   "source": [
    "def add_specialized_tokens_to_sentences(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Add specialized tokens for each line (sentence) in the text.\n",
    "    Each newline is treated as a sentence boundary.\n",
    "    Only adds <|endoftext|> token at the end of each sentence.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to process\n",
    "        \n",
    "    Returns:\n",
    "        Text with <|endoftext|> tokens added for each sentence\n",
    "    \"\"\"\n",
    "    # Define specialized token (following GPT-2 style)\n",
    "    END_OF_TEXT = \"<|endoftext|>\"\n",
    "    \n",
    "    # Split text into lines (each line is a sentence)\n",
    "    lines = text.split('\\n')\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # Only process non-empty lines\n",
    "            # Add only end of text token\n",
    "            sentence_with_tokens = f\"{line} {END_OF_TEXT}\"\n",
    "            processed_sentences.append(sentence_with_tokens)\n",
    "    \n",
    "    return '\\n'.join(processed_sentences)\n",
    "\n",
    "# Apply specialized tokens to your final_text\n",
    "print(\"Adding <|endoftext|> tokens to final_text (treating each line as a sentence)...\")\n",
    "final_text_with_tokens = add_specialized_tokens_to_sentences(final_text)\n",
    "\n",
    "# Save the processed text\n",
    "output_file_with_tokens = \"../data/training_data_with_special_tokens.txt\"\n",
    "with open(output_file_with_tokens, 'w', encoding='utf-8') as f:\n",
    "    f.write(final_text_with_tokens)\n",
    "\n",
    "print(f\"Original text length: {len(final_text)} characters\")\n",
    "print(f\"Text with special tokens length: {len(final_text_with_tokens)} characters\")\n",
    "\n",
    "# Count lines/sentences\n",
    "original_lines = len([line for line in final_text.split('\\n') if line.strip()])\n",
    "processed_lines = len([line for line in final_text_with_tokens.split('\\n') if line.strip()])\n",
    "print(f\"Number of sentences (lines): {original_lines}\")\n",
    "\n",
    "# Test tokenization with the new text\n",
    "from src.data.tokenizer import get_tokenizer, text_to_token_ids\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tokens_with_special = text_to_token_ids(final_text_with_tokens, tokenizer)\n",
    "\n",
    "print(f\"Number of tokens with special tokens: {len(tokens_with_special[0])}\")\n",
    "\n",
    "# Show a sample of the processed text\n",
    "sample_lines = final_text_with_tokens.split('\\n')[:5]  # First 5 sentences\n",
    "print(\"\\nSample of first 5 sentences with <|endoftext|> tokens:\")\n",
    "for i, line in enumerate(sample_lines, 1):\n",
    "    print(f\"{i}. {line}\")\n",
    "\n",
    "# Check how the special tokens are tokenized\n",
    "special_token_sample = \"Hello world. <|endoftext|>\"\n",
    "special_tokens = text_to_token_ids(special_token_sample, tokenizer)\n",
    "print(f\"\\nSpecial token sample: '{special_token_sample}'\")\n",
    "print(f\"Tokenized as: {special_tokens[0].tolist()}\")\n",
    "print(f\"Decoded back: '{tokenizer.decode(special_tokens[0].tolist())}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86f31f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 9297503\n",
      "Number of tokens: 13784480\n"
     ]
    }
   ],
   "source": [
    "# tokenize the data for some analysis\n",
    "from src.data.tokenizer import get_tokenizer, text_to_token_ids\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "tokens = text_to_token_ids(final_text_with_tokens, tokenizer)\n",
    "\n",
    "print(f\"Number of words: {len(final_text_with_tokens.split())}\")\n",
    "print(f\"Number of tokens: {len(tokens[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a049d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 45201\n",
      "Most frequent token: 220:  (appears 852775 times)\n",
      "Least frequent token: 98:� (appears 1 times)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "token_array = tokens[0].numpy()\n",
    "\n",
    "unique_tokens, counts = np.unique(token_array, return_counts=True)\n",
    "print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "mft = unique_tokens[np.argmax(counts)]\n",
    "lft = unique_tokens[np.argmin(counts)]\n",
    "print(f\"Most frequent token: {mft}:{tokenizer.decode([mft])} (appears {counts.max()} times)\")\n",
    "print(f\"Least frequent token: {lft}:{tokenizer.decode([lft])} (appears {counts.min()} times)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fffd059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent tokens:\n",
      " 1. Token ID:   220 | Text: ' ' | Count:   852775 | Percentage:  6.19%\n",
      " 2. Token ID: 50256 | Text: '<|endoftext|>' | Count:   815911 | Percentage:  5.92%\n",
      " 3. Token ID:   198 | Text: '\n",
      "' | Count:   815910 | Percentage:  5.92%\n",
      " 4. Token ID:    13 | Text: '.' | Count:   664473 | Percentage:  4.82%\n",
      " 5. Token ID:    11 | Text: ',' | Count:   462790 | Percentage:  3.36%\n",
      " 6. Token ID:   262 | Text: ' the' | Count:   354518 | Percentage:  2.57%\n",
      " 7. Token ID:   284 | Text: ' to' | Count:   189168 | Percentage:  1.37%\n",
      " 8. Token ID:   257 | Text: ' a' | Count:   178461 | Percentage:  1.29%\n",
      " 9. Token ID:   345 | Text: ' you' | Count:   176208 | Percentage:  1.28%\n",
      "10. Token ID:   290 | Text: ' and' | Count:   171039 | Percentage:  1.24%\n",
      "11. Token ID:    30 | Text: '?' | Count:   162007 | Percentage:  1.18%\n",
      "12. Token ID:   286 | Text: ' of' | Count:   156721 | Percentage:  1.14%\n",
      "13. Token ID:   338 | Text: ''s' | Count:   139308 | Percentage:  1.01%\n",
      "14. Token ID:   287 | Text: ' in' | Count:   124853 | Percentage:  0.91%\n",
      "15. Token ID:   340 | Text: ' it' | Count:   118509 | Percentage:  0.86%\n",
      "16. Token ID:   314 | Text: ' I' | Count:   114471 | Percentage:  0.83%\n",
      "17. Token ID:   326 | Text: ' that' | Count:   110627 | Percentage:  0.80%\n",
      "18. Token ID:    40 | Text: 'I' | Count:    88673 | Percentage:  0.64%\n",
      "19. Token ID:   373 | Text: ' was' | Count:    81883 | Percentage:  0.59%\n",
      "20. Token ID:   318 | Text: ' is' | Count:    80909 | Percentage:  0.59%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from src.data.tokenizer import token_ids_to_text\n",
    "import torch\n",
    "\n",
    "# Get top 20 most frequent tokens\n",
    "token_counter = Counter(token_array)\n",
    "top_tokens = token_counter.most_common(20)\n",
    "\n",
    "print(\"Top 20 most frequent tokens:\")\n",
    "for i, (token_id, count) in enumerate(top_tokens, 1):\n",
    "    token_text = token_ids_to_text(torch.tensor([[token_id]]), tokenizer)\n",
    "    percentage = (count / len(token_array)) * 100\n",
    "    print(f\"{i:2d}. Token ID: {token_id:5d} | Text: '{token_text}' | Count: {count:8d} | Percentage: {percentage:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04f395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
