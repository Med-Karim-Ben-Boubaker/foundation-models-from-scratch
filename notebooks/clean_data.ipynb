{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4ee72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: ../data/simple_wiki_cleaned.train\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "input_file = \"../data/simple_wiki.train\"    \n",
    "output_file = \"../data/simple_wiki_cleaned.train\"\n",
    "\n",
    "with open(input_file, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "lines = content.split('\\n')\n",
    "cleaned_lines = []\n",
    "\n",
    "for line in lines:\n",
    "    if not line.strip().startswith('= = ='):\n",
    "        cleaned_lines.append(line.strip())\n",
    "\n",
    "# Join all lines without newlines\n",
    "result = ''.join(cleaned_lines)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(result)\n",
    "\n",
    "print(f\"Cleaned data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86f31f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 1310381\n",
      "Number of tokens: 1832887\n"
     ]
    }
   ],
   "source": [
    "# tokenize the data for some analysis\n",
    "from src.data.tokenizer import get_tokenizer, text_to_token_ids\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "tokens = text_to_token_ids(result, tokenizer)\n",
    "\n",
    "print(f\"Number of words: {len(result.split())}\")\n",
    "print(f\"Number of tokens: {len(tokens[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049d623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 38793\n",
      "Most frequent token: 13:. (appears 92567 times)\n",
      "Least frequent token: 104:ï¿½ (appears 1 times)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "token_array = tokens[0].numpy()\n",
    "\n",
    "unique_tokens, counts = np.unique(token_array, return_counts=True)\n",
    "print(f\"Unique tokens: {len(unique_tokens)}\")\n",
    "mft = unique_tokens[np.argmax(counts)]\n",
    "lft = unique_tokens[np.argmin(counts)]\n",
    "print(f\"Most frequent token: {mft}:{tokenizer.decode([mft])} (appears {counts.max()} times)\")\n",
    "print(f\"Least frequent token: {lft}:{tokenizer.decode([lft])} (appears {counts.min()} times)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fffd059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent tokens:\n",
      " 1. Token ID:    13 | Text: '.' | Count:    92567 | Percentage:  5.05%\n",
      " 2. Token ID:   262 | Text: ' the' | Count:    78173 | Percentage:  4.27%\n",
      " 3. Token ID:    11 | Text: ',' | Count:    68567 | Percentage:  3.74%\n",
      " 4. Token ID:   286 | Text: ' of' | Count:    44572 | Percentage:  2.43%\n",
      " 5. Token ID:   287 | Text: ' in' | Count:    39220 | Percentage:  2.14%\n",
      " 6. Token ID:   290 | Text: ' and' | Count:    34393 | Percentage:  1.88%\n",
      " 7. Token ID:   257 | Text: ' a' | Count:    29133 | Percentage:  1.59%\n",
      " 8. Token ID:   318 | Text: ' is' | Count:    25858 | Percentage:  1.41%\n",
      " 9. Token ID:   284 | Text: ' to' | Count:    22592 | Percentage:  1.23%\n",
      "10. Token ID:   373 | Text: ' was' | Count:    22345 | Percentage:  1.22%\n",
      "11. Token ID:   366 | Text: ' \"' | Count:    14348 | Percentage:  0.78%\n",
      "12. Token ID:    12 | Text: '-' | Count:    12922 | Percentage:  0.71%\n",
      "13. Token ID:   357 | Text: ' (' | Count:    12112 | Percentage:  0.66%\n",
      "14. Token ID:   329 | Text: ' for' | Count:    10539 | Percentage:  0.57%\n",
      "15. Token ID:   319 | Text: ' on' | Count:    10210 | Percentage:  0.56%\n",
      "16. Token ID:   383 | Text: ' The' | Count:     9437 | Percentage:  0.51%\n",
      "17. Token ID:   355 | Text: ' as' | Count:     8279 | Percentage:  0.45%\n",
      "18. Token ID:   416 | Text: ' by' | Count:     8078 | Percentage:  0.44%\n",
      "19. Token ID:     1 | Text: '\"' | Count:     7899 | Percentage:  0.43%\n",
      "20. Token ID:   422 | Text: ' from' | Count:     7723 | Percentage:  0.42%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from src.data.tokenizer import token_ids_to_text\n",
    "import torch\n",
    "\n",
    "# Get top 20 most frequent tokens\n",
    "token_counter = Counter(token_array)\n",
    "top_tokens = token_counter.most_common(20)\n",
    "\n",
    "print(\"Top 20 most frequent tokens:\")\n",
    "for i, (token_id, count) in enumerate(top_tokens, 1):\n",
    "    token_text = token_ids_to_text(torch.tensor([[token_id]]), tokenizer)\n",
    "    percentage = (count / len(token_array)) * 100\n",
    "    print(f\"{i:2d}. Token ID: {token_id:5d} | Text: '{token_text}' | Count: {count:8d} | Percentage: {percentage:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04f395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
