
# gpt2_35m_4heads_12layers
model:
  vocab_size: 50257
  context_length: 512
  emb_dim: 256
  n_heads: 4
  n_layers: 12
  drop_rate: 0.1
  qkv_bias: false

train:
  batch_size: 4
  lr: 0.002
  weight_decay: 0.01
  num_epochs: 1
  eval_freq: 1000
  eval_iter: 50
  grad_accum_steps: 2
  amp: true
  device: cuda
  seed: 123
  num_workers: 2
  warmup_steps: 1000
  min_lr: 1.0e-5
  betas: [0.9, 0.999]
  eps: 1.0e-8
  fused: true
  grad_clip_norm: 1.0


# model:
#   vocab_size: 50257
#   context_length: 512
#   emb_dim: 512        # Up from 256 - escapes softmax bottleneck
#   n_heads: 8          # Up from 4 - better attention coverage
#   n_layers: 9         # Down from 12 - first layers matter most
#   drop_rate: 0.1      # Remove dropout for pretraining
#   qkv_bias: false

# train:
#   batch_size: 4
#   lr: 0.0005          # Reduced from 0.002 - CRITICAL FIX
#   weight_decay: 0.01
#   num_epochs: 1       # Train longer on existing data
#   eval_freq: 1000
#   eval_iter: 50
#   grad_accum_steps: 2
#   amp: true
#   device: cuda
#   seed: 123
#   num_workers: 2
#   warmup_steps: 2000  # Increased from 1000
#   min_lr: 1.0e-6
#   betas: [0.9, 0.9999]  # Fixed beta2 for small batch size
#   eps: 1.0e-8
#   fused: true
#   grad_clip_norm: 1.0